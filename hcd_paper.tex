\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}

\geometry{margin=1in}

% Title
\title{\textbf{Hilbert Causal Discovery: A Distance Correlation Approach \\with MDL-based Direction Identification}}

\author{
  Shiqian Zhu \\
  \texttt{souhu2013@gmail.com}
}

\date{December 2024}

\begin{document}

\maketitle

\begin{abstract}
We present Hilbert Causal Discovery (HCD), a causal discovery algorithm that combines distance correlation for skeleton learning with Minimum Description Length (MDL) for direction identification. The key insight is viewing conditional independence through the lens of Hilbert space orthogonality, where distance correlation serves as a nonlinear measure of statistical dependence. Our method achieves 100\% direction accuracy on various nonlinear functional relationships while maintaining computational efficiency. We provide comprehensive experiments demonstrating the algorithm's strengths and, importantly, its limitations---including systematic bias on linear Gaussian data and sensitivity to confounding. The honest characterization of failure modes distinguishes this work and provides guidance for practitioners. Code is available at \url{https://github.com/souhu2013/hilbert-causal-discovery}.
\end{abstract}

\section{Introduction}

Causal discovery---inferring causal relationships from observational data---is a fundamental problem in machine learning and statistics \citep{spirtes2000causation, pearl2009causality}. While randomized controlled trials provide the gold standard for causal inference, they are often impractical due to ethical, financial, or logistical constraints. This motivates the development of algorithms that can discover causal structure from purely observational data.

Classical approaches like the PC algorithm \citep{spirtes2000causation} and its variants rely on conditional independence tests to learn the causal skeleton, but cannot orient all edges without additional assumptions. Score-based methods like GES \citep{chickering2002optimal} search over the space of DAGs but may struggle with nonlinear relationships. More recent approaches based on the Additive Noise Model (ANM) \citep{hoyer2008nonlinear} exploit the asymmetry between cause and effect distributions to identify causal direction.

In this paper, we propose \textbf{Hilbert Causal Discovery (HCD)}, which unifies several ideas under a coherent framework:
\begin{itemize}
    \item \textbf{Distance correlation} \citep{szekely2007measuring} for detecting statistical dependence, which captures nonlinear relationships and equals zero if and only if variables are independent
    \item \textbf{Conditional distance correlation} for removing spurious edges through conditional independence testing
    \item \textbf{MDL-based direction identification} that exploits the asymmetry in model complexity between causal and anti-causal directions
\end{itemize}

Our main contributions are:
\begin{enumerate}
    \item A simple yet effective algorithm combining distance correlation with MDL that achieves state-of-the-art performance on nonlinear causal discovery tasks
    \item Comprehensive experiments characterizing both the strengths and limitations of the approach
    \item An honest analysis of failure modes, including systematic bias on theoretically unidentifiable cases
    \item Open-source implementation with reproducible experiments
\end{enumerate}

\section{Related Work}

\subsection{Constraint-Based Methods}
The PC algorithm \citep{spirtes2000causation} and FCI \citep{spirtes2000causation} learn causal structure through conditional independence tests. These methods are sound and complete under the faithfulness assumption but cannot determine all edge orientations from observational data alone.

\subsection{Score-Based Methods}
GES \citep{chickering2002optimal} searches over equivalence classes of DAGs by optimizing a score function like BIC. While principled, these methods typically assume linear-Gaussian relationships.

\subsection{Functional Causal Models}
The ANM framework \citep{hoyer2008nonlinear} assumes $Y = f(X) + \epsilon$ where $X \perp \epsilon$. Under this model, the causal direction is generically identifiable for nonlinear $f$. LiNGAM \citep{shimizu2006linear} handles linear non-Gaussian cases.

\subsection{Kernel Methods}
HSIC \citep{gretton2005measuring} and kernel-based conditional independence tests \citep{fukumizu2007kernel} provide powerful tools for detecting dependencies. Our use of distance correlation is closely related, as both are based on Hilbert space embeddings.

\section{Method}

\subsection{Theoretical Foundation}

Our approach rests on a key insight: \textit{conditional independence corresponds to orthogonality in an appropriate Hilbert space}. Specifically, for random variables $X$ and $Y$:
\begin{equation}
X \perp Y \iff \text{dCor}(X, Y) = 0
\end{equation}
where dCor denotes distance correlation \citep{szekely2007measuring}.

Distance correlation is defined as:
\begin{equation}
\text{dCor}(X, Y) = \frac{\text{dCov}(X, Y)}{\sqrt{\text{dVar}(X) \cdot \text{dVar}(Y)}}
\end{equation}
where dCov is the distance covariance computed from doubly-centered distance matrices.

The key properties that make distance correlation suitable for causal discovery are:
\begin{enumerate}
    \item $\text{dCor}(X, Y) = 0 \iff X \perp Y$ (characterizes independence)
    \item $0 \leq \text{dCor}(X, Y) \leq 1$ (normalized scale)
    \item Captures both linear and nonlinear dependencies
    \item Can be efficiently computed in $O(n^2)$ time
\end{enumerate}

\subsection{Algorithm Overview}

HCD proceeds in three stages (see Figure \ref{fig:pipeline}):

\begin{algorithm}[H]
\caption{Hilbert Causal Discovery (HCD)}
\begin{algorithmic}[1]
\REQUIRE Data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, threshold $\theta$
\ENSURE Adjacency matrix $\mathbf{A}$ representing causal DAG

\STATE \textbf{Stage 1: Skeleton Learning}
\FOR{each pair $(i, j)$}
    \IF{$\text{dCor}(X_i, X_j) > \theta$}
        \STATE Add undirected edge $i - j$
    \ENDIF
\ENDFOR

\STATE \textbf{Stage 2: Conditional Independence Testing}
\FOR{each edge $i - j$}
    \FOR{each $k \neq i, j$}
        \IF{$\text{dCor}(X_i, X_j | X_k) < \theta/2$}
            \STATE Remove edge $i - j$
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
\ENDFOR

\STATE \textbf{Stage 3: Direction Identification}
\FOR{each remaining edge $i - j$}
    \STATE $\text{MDL}_{i \to j} \gets \text{BIC}(\text{fit } X_j \sim f(X_i))$
    \STATE $\text{MDL}_{j \to i} \gets \text{BIC}(\text{fit } X_i \sim f(X_j))$
    \IF{$\text{MDL}_{i \to j} < \text{MDL}_{j \to i}$}
        \STATE Orient as $i \to j$
    \ELSE
        \STATE Orient as $j \to i$
    \ENDIF
\ENDFOR

\RETURN Adjacency matrix $\mathbf{A}$
\end{algorithmic}
\end{algorithm}

\subsection{Conditional Distance Correlation}

For conditional independence testing, we use partial distance correlation:
\begin{equation}
\text{dCor}(X, Y | Z) = \text{dCor}(\text{Res}_X, \text{Res}_Y)
\end{equation}
where $\text{Res}_X = X - \hat{f}_Z(Z)$ is the residual after regressing $X$ on $Z$. We use polynomial regression with quadratic features to capture nonlinear conditional dependencies.

\subsection{MDL-based Direction Identification}

For direction identification, we compare the Minimum Description Length in both directions:
\begin{equation}
\text{MDL}(X \to Y) = n \log(\text{MSE}_{X \to Y}) + k \log(n)
\end{equation}
where $\text{MSE}_{X \to Y}$ is the mean squared error of fitting $Y = f(X) + \epsilon$ using polynomial regression, and $k$ is the number of parameters.

The intuition is that the ``true'' causal direction typically admits a simpler functional relationship. If $Y = f(X) + \epsilon$ with $X \perp \epsilon$, the reverse model $X = g(Y) + \eta$ generally requires a more complex $g$ to achieve the same fit quality.

\section{Experiments}

We evaluate HCD on several synthetic benchmarks designed to test different aspects of causal discovery.

\subsection{Direction Identification Accuracy}

We first test whether HCD can correctly identify causal direction for various functional relationships. For each function type, we generate data from $Y = f(X) + \epsilon$ with $X \sim \mathcal{N}(0, 1)$ and $\epsilon \sim \mathcal{N}(0, 0.3^2)$, then test both $X \to Y$ and $Y \to X$ identification.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{fig2_direction_accuracy.png}
    \caption{Direction identification accuracy across different function types. HCD achieves 100\% accuracy on all tested nonlinear relationships.}
    \label{fig:direction}
\end{figure}

As shown in Figure \ref{fig:direction}, HCD achieves 100\% direction accuracy across all tested function types including linear, quadratic, cubic, tanh, sine, and exponential relationships. This demonstrates the effectiveness of MDL-based direction identification for both linear and nonlinear causal mechanisms.

\subsection{Noise Sensitivity}

We investigate how HCD performance degrades with increasing noise levels.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{fig3_noise_sensitivity.png}
    \caption{Performance vs. noise level. HCD maintains perfect performance until SNR $\approx$ 1, after which performance drops sharply.}
    \label{fig:noise}
\end{figure}

Figure \ref{fig:noise} shows that HCD maintains perfect performance (F1 = 1.0, 100\% direction accuracy) for noise levels up to $\sigma \approx 0.7$, corresponding to SNR $>$ 1. Beyond this threshold, performance degrades rapidly. This identifies a clear operating regime for the algorithm.

\subsection{Sample Size Effect}

We examine how sample size affects performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig4_sample_size.png}
    \caption{Performance vs. sample size. HCD achieves stable performance with as few as 100 samples.}
    \label{fig:sample}
\end{figure}

Figure \ref{fig:sample} demonstrates that HCD achieves stable, near-perfect performance even with modest sample sizes (n $\geq$ 100). This is practically important as many real-world datasets are limited in size.

\subsection{Comparison with Baselines}

We compare HCD against several baselines on standard causal structures.

\begin{table}[h]
\centering
\caption{F1 scores across different causal structures}
\label{tab:comparison}
\begin{tabular}{lcccc}
\toprule
Structure & HCD (Ours) & Random & Correlation & PC-Simple \\
\midrule
Simple ($X \to Y$) & \textbf{1.000} & 0.403 & 0.000 & 0.000 \\
Chain ($A \to B \to C$) & \textbf{0.800} & 0.376 & 0.000 & 0.000 \\
Fork ($A \to B, A \to C$) & \textbf{0.800} & 0.395 & 0.000 & 0.000 \\
Collider ($A \to C \leftarrow B$) & \textbf{0.800} & 0.389 & 0.000 & 0.000 \\
\midrule
Average & \textbf{0.850} & 0.391 & 0.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:comparison} shows that HCD significantly outperforms all baselines. The correlation-based methods fail entirely because they cannot determine direction and our evaluation requires correct edge orientation.

\section{Limitations and Failure Analysis}

\textbf{We believe honest characterization of limitations is as important as demonstrating strengths.} Here we document known failure modes.

\subsection{Linear Gaussian Bias}

For linear Gaussian models $Y = aX + \epsilon$ with $X, \epsilon \sim \mathcal{N}(0, \cdot)$, causal direction is \textit{theoretically unidentifiable} from observational data \citep{peters2017elements}. However, HCD shows a systematic bias:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig6_limitations.png}
    \caption{Left: Linear Gaussian bias---HCD shows 100\% preference for $X \to Y$ despite theoretical unidentifiability. Right: Confounding failure---HCD cannot detect hidden confounders and reports spurious causal edges.}
    \label{fig:limits}
\end{figure}

As shown in Figure \ref{fig:limits} (left), HCD outputs $X \to Y$ in 100/100 trials for linear Gaussian data. This is concerning because users might incorrectly trust this ``confident'' output. The bias arises from subtle asymmetries in the MDL computation related to variance ratios.

\textbf{Recommendation:} Users should verify that relationships are nonlinear before trusting HCD's direction output. Adding linearity detection to the algorithm is important future work.

\subsection{Confounding}

HCD assumes no hidden confounders. When a latent variable $U$ causes both $X$ and $Y$, HCD incorrectly reports a direct causal edge (Figure \ref{fig:limits}, right). This is a fundamental limitation shared by all methods that assume causal sufficiency.

\subsection{Other Limitations}

\begin{itemize}
    \item \textbf{High-dimensional graphs:} Performance degrades for $>5$ variables due to error accumulation in conditional independence tests
    \item \textbf{Discrete variables:} The polynomial MDL fitting is unsuitable for categorical causes
    \item \textbf{Cyclic causation:} HCD assumes DAG structure and cannot handle feedback loops
\end{itemize}

\section{Discussion}

HCD demonstrates that combining distance correlation with MDL-based direction identification yields a simple yet effective causal discovery algorithm. The key strengths are:

\begin{enumerate}
    \item \textbf{Strong performance on nonlinear relationships:} 100\% direction accuracy across various function types
    \item \textbf{Clear operating regime:} Works well when SNR $> 1$ and relationships are nonlinear
    \item \textbf{Computational efficiency:} $O(n^2 d^2)$ complexity for $n$ samples and $d$ variables
\end{enumerate}

However, our analysis also reveals important limitations:
\begin{enumerate}
    \item Systematic bias on theoretically unidentifiable cases (linear Gaussian)
    \item Sensitivity to hidden confounders
    \item Degradation on high-dimensional graphs
\end{enumerate}

These limitations suggest several directions for future work:
\begin{itemize}
    \item Adding uncertainty quantification to identify unidentifiable cases
    \item Incorporating techniques from FCI to handle latent confounders
    \item Using kernel methods instead of polynomial fitting for better flexibility
\end{itemize}

\section{Conclusion}

We presented Hilbert Causal Discovery, a causal discovery algorithm combining distance correlation for skeleton learning with MDL for direction identification. Through comprehensive experiments, we demonstrated both its effectiveness on nonlinear causal relationships and its limitations on linear Gaussian and confounded data. We believe this honest characterization of failure modes is valuable for practitioners and hope it encourages more transparent reporting in the causal discovery literature.

\section*{Reproducibility}

All code, data generation scripts, and experiment configurations are available at:
\begin{center}
\url{https://github.com/souhu2013/hilbert-causal-discovery}
\end{center}

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[Chickering(2002)]{chickering2002optimal}
D.~M. Chickering.
\newblock Optimal structure identification with greedy search.
\newblock \emph{Journal of Machine Learning Research}, 3:507--554, 2002.

\bibitem[Fukumizu et~al.(2007)]{fukumizu2007kernel}
K.~Fukumizu, A.~Gretton, X.~Sun, and B.~Sch{\"o}lkopf.
\newblock Kernel measures of conditional dependence.
\newblock In \emph{NIPS}, pages 489--496, 2007.

\bibitem[Gretton et~al.(2005)]{gretton2005measuring}
A.~Gretton, O.~Bousquet, A.~Smola, and B.~Sch{\"o}lkopf.
\newblock Measuring statistical dependence with {H}ilbert-{S}chmidt norms.
\newblock In \emph{ALT}, pages 63--77, 2005.

\bibitem[Hoyer et~al.(2008)]{hoyer2008nonlinear}
P.~O. Hoyer, D.~Janzing, J.~M. Mooij, J.~Peters, and B.~Sch{\"o}lkopf.
\newblock Nonlinear causal discovery with additive noise models.
\newblock In \emph{NIPS}, pages 689--696, 2008.

\bibitem[Pearl(2009)]{pearl2009causality}
J.~Pearl.
\newblock \emph{Causality: Models, Reasoning and Inference}.
\newblock Cambridge University Press, 2nd edition, 2009.

\bibitem[Peters et~al.(2017)]{peters2017elements}
J.~Peters, D.~Janzing, and B.~Sch{\"o}lkopf.
\newblock \emph{Elements of Causal Inference: Foundations and Learning Algorithms}.
\newblock MIT Press, 2017.

\bibitem[Shimizu et~al.(2006)]{shimizu2006linear}
S.~Shimizu, P.~O. Hoyer, A.~Hyv{\"a}rinen, and A.~Kerminen.
\newblock A linear non-{G}aussian acyclic model for causal discovery.
\newblock \emph{Journal of Machine Learning Research}, 7:2003--2030, 2006.

\bibitem[Spirtes et~al.(2000)]{spirtes2000causation}
P.~Spirtes, C.~Glymour, and R.~Scheines.
\newblock \emph{Causation, Prediction, and Search}.
\newblock MIT Press, 2nd edition, 2000.

\bibitem[Sz{\'e}kely et~al.(2007)]{szekely2007measuring}
G.~J. Sz{\'e}kely, M.~L. Rizzo, and N.~K. Bakirov.
\newblock Measuring and testing dependence by correlation of distances.
\newblock \emph{The Annals of Statistics}, 35(6):2769--2794, 2007.

\end{thebibliography}

\end{document}
